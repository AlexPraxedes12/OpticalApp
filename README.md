# OpticalApp

OpticalApp is a Flutter-based mobile application submitted to the James Dyson Awards. It provides fully offline ocular disease screening using a TensorFlow Lite model.

## Project Description
OpticalApp helps communities with limited access to ophthalmologists by allowing non-experts to screen for retinal diseases using just a smartphone. The app loads a lightweight EfficientNet-B0 model to classify fundus images directly on-device without internet connectivity.

## Features
- Classifies **28** retinal diseases from fundus photographs
- Works completely **offline**
- Provides textual diagnosis and symptom descriptions
- Optional voice output via text-to-speech
- Supports **Spanish**, **English**, and **French**
- Capture images with front or back camera or select from gallery
- Accessible interface for quick interpretation

## Model Training and Conversion
The EfficientNet-B0 model was trained on the **RFMiD** (Retinal Fundus Multi-disease Image Dataset) using PyTorch in Google Colab. The resulting `.pth` weights were exported to ONNX and then converted to TensorFlow Lite. Conversion steps were performed partially in Colab and partially on a local machine. The final `model_float32.tflite` file is included in `assets/` alongside the labels file.

## Offline Capabilities
All inference runs on the device with no network calls. This makes the app suitable for remote locations and ensures user images remain private.

## Multilingual Support
UI strings, condition names, descriptions, and text-to-speech voices are provided for **en**, **es**, and **fr**. The current language can be changed from a simple dropdown on each screen.

## How to Run the App
1. Install Flutter (version 3.10 or later) and run `flutter pub get`.
2. Connect an Android or iOS device or start an emulator.
3. Execute `flutter run` from the project root. The TFLite model in `assets/` will be bundled automatically.

## How to Retrain the Model
1. See the scripts and notebooks under `model_training/efficientnet/`.
2. Train `train_efficientnet.py` or the provided Colab notebook with the RFMiD dataset.
3. Export the PyTorch weights to ONNX and run `convert_onnx_to_tflite.py` to produce a new `.tflite` model.
4. Replace the model file in `assets/` and update the labels as needed.

## Folder Structure
- `lib/` – Flutter source code
- `assets/` – TFLite models, labels, and images
- `model_training/` – Training scripts and conversion utilities
- `android/`, `ios/`, `linux/`, `macos/`, `windows/`, `web/` – Platform-specific folders generated by Flutter

## Licensing / Credits
- RFMiD dataset courtesy of [Ritsumeikan University](https://www.kaggle.com/datasets/rishabhiitbhu/rfmid-retinal-disease-classification). 
- EfficientNet model from [Google AI](https://arxiv.org/abs/1905.11946). 
- This project is provided for academic and humanitarian purposes. Please contact the authors for other uses.

## Notes on Mobile Fundus Photography and Future Work
Capturing quality retinal images with a phone can be challenging due to motion blur, improper lighting, or limited field of view. Future versions aim to integrate guidance for alignment, improve preprocessing, and explore lighter models for faster inference on low-end devices.
